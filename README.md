# Q-A-using-RAG

This project demonstrates the integration of various tools to build a text generation and document retrieval system using LangChain, Hugging Face models, and FAISS for efficient vector storage. The project begins by setting up a Hugging Face pipeline for text generation using GPT-2, which is wrapped with LangChainâ€™s HuggingFacePipeline to enable seamless interaction. It loads a CSV file containing FAQ data and uses the EmbaasEmbeddings class from LangChain Community to convert query data into embeddings, facilitating efficient document retrieval. These embeddings are stored and indexed using FAISS, which allows fast retrieval of relevant documents. A custom prompt template is then used to generate answers based on the retrieved context, ensuring the response is derived from the available document. The retrieval and answer generation process is combined into a LangChain RetrievalQA chain, which enables the system to answer questions by querying the relevant context and generating meaningful responses. This system provides a solid foundation for creating advanced question-answering applications that can handle large datasets and complex queries efficiently.
